{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdcac30c-59b9-4108-909d-3fc244fffb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20a8ea86-ef84-4c3d-bc58-ae9b76150fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELM:\n",
    "    \"\"\"\n",
    "    Extreme Learning Machine for a single-hidden-layer feedforward neural network (SLFN).\n",
    "    Steps (Huang et al., 2006):\n",
    "      1) Randomly assign input weights W1 and biases b for the hidden layer.\n",
    "      2) Compute hidden layer output matrix H.\n",
    "      3) Compute output weights beta = H^dagger * T (Moore-Penrose pseudoinverse).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='relu', seed=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            Number of input features (dimension of x).\n",
    "        hidden_size : int\n",
    "            Number of hidden neurons.\n",
    "        output_size : int\n",
    "            Number of output dimensions (dimension of t).\n",
    "        activation : callable, optional\n",
    "            Activation function g(z) to use.\n",
    "            If None, defaults to ReLU.\n",
    "        seed : int, optional\n",
    "            Seed for reproducible random initialization.\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Choose activation\n",
    "        if isinstance(activation, str):\n",
    "            if activation.lower() == 'relu':\n",
    "                self.activation = self._relu\n",
    "                self.activation_deriv = self._relu_derivative\n",
    "            elif activation.lower() == 'tanh':\n",
    "                self.activation = self._tanh\n",
    "                self.activation_deriv = self._tanh_derivative\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported activation string. Use 'relu' or 'tanh'.\")\n",
    "        else:\n",
    "            # user-supplied function\n",
    "            self.activation = activation\n",
    "            # no derivative provided => handle carefully or raise error\n",
    "            self.activation_deriv = None\n",
    "            print(\"Warning: no derivative for a custom activation. Backprop may fail.\")\n",
    "\n",
    "        # Randomly init input->hidden weights, not updated in ELM\n",
    "        # For ReLU, a good approach is He initialization:\n",
    "        # For Tanh, a good approach is Xavier (scaled uniform).\n",
    "        if isinstance(activation, str) and activation.lower() == 'relu':\n",
    "            # He initialization\n",
    "            he_std = np.sqrt(2.0 / input_size)\n",
    "            self.weights_input_hidden = np.random.normal(\n",
    "                loc=0.0, scale=he_std, size=(input_size, hidden_size)\n",
    "            )\n",
    "            self.bias_hidden = np.random.normal(\n",
    "                loc=0.0, scale=1e-2, size=(1, hidden_size)\n",
    "            )\n",
    "        else:\n",
    "            # e.g. Tanh => Xavier\n",
    "            limit = np.sqrt(6.0 / (input_size + hidden_size))\n",
    "            self.weights_input_hidden = np.random.uniform(\n",
    "                low=-limit, high=limit, size=(input_size, hidden_size)\n",
    "            )\n",
    "            self.bias_hidden = np.random.uniform(\n",
    "                low=-limit, high=limit, size=(1, hidden_size)\n",
    "            )\n",
    "\n",
    "        # Hidden->output weights: We DO train these\n",
    "        # We'll do a simple Xavier-like approach for either ReLU or Tanh:\n",
    "        limit_out = np.sqrt(6.0 / (hidden_size + output_size))\n",
    "        self.weights_hidden_output = np.random.uniform(\n",
    "            low=-limit_out, high=limit_out, size=(hidden_size, output_size)\n",
    "        )\n",
    "        self.bias_output = np.random.uniform(\n",
    "            low=-limit_out, high=limit_out, size=(1, output_size)\n",
    "        )\n",
    "\n",
    "        # Placeholders for forward pass\n",
    "        self.hidden_layer_input = None\n",
    "        self.hidden_layer_output = None\n",
    "        self.output_layer_input = None\n",
    "        self.predicted_output = None\n",
    "\n",
    "    # ------------------------\n",
    "    # Activation functions\n",
    "    # ------------------------\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        # derivative wrt the pre-activation\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _tanh_derivative(self, x):\n",
    "        # derivative wrt pre-activation for tanh\n",
    "        # if we define y = tanh(x), derivative = 1 - y^2\n",
    "        y = np.tanh(x)\n",
    "        return 1.0 - y*y\n",
    "\n",
    "    # L1 subgradient\n",
    "    def _l1_subgrad(self, w):\n",
    "        # returns sign(w), with sign(0)=0\n",
    "        grad = np.zeros_like(w)\n",
    "        grad[w > 0] = 1.0\n",
    "        grad[w < 0] = -1.0\n",
    "        return grad\n",
    "\n",
    "    # ------------------------\n",
    "    # Forward pass\n",
    "    # ------------------------\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass with either ReLU or tanh hidden activation,\n",
    "        then a linear activation (or if you prefer, you could\n",
    "        also do tanh at the output).\n",
    "        \"\"\"\n",
    "        # hidden\n",
    "        self.hidden_layer_input = X.dot(self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_layer_output = self.activation(self.hidden_layer_input)\n",
    "\n",
    "        # output\n",
    "        self.output_layer_input = self.hidden_layer_output.dot(self.weights_hidden_output) + self.bias_output\n",
    "        # We'll do linear output by default.\n",
    "        # If you want tanh final, do: self.predicted_output = np.tanh(self.output_layer_input)\n",
    "        self.predicted_output = self.output_layer_input\n",
    "\n",
    "        return self.predicted_output\n",
    "\n",
    "    # ------------------------\n",
    "    # Backward pass\n",
    "    # ------------------------\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute gradients wrt the hidden->output weights\n",
    "        for a MSE + L1 penalty on W2.\n",
    "\n",
    "        Because we do not update input->hidden in an ELM\n",
    "        (by definition it's random and fixed),\n",
    "        we only compute partial derivatives wrt (W2, b2).\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # 1) dLoss/d(output)\n",
    "        # MSE derivative: (pred - y)\n",
    "        output_error = (self.predicted_output - y)  # shape (n_samples, output_size)\n",
    "\n",
    "        # 2) derivative wrt W2, b2\n",
    "        # hidden_layer_output shape: (n_samples, hidden_size)\n",
    "        dW2 = (self.hidden_layer_output.T @ output_error) / n_samples\n",
    "        db2 = np.sum(output_error, axis=0, keepdims=True) / n_samples\n",
    "\n",
    "        # L1 subgradient on W2\n",
    "        if self.l1_lambda > 1e-15:\n",
    "            dW2 += self.l1_lambda * self._l1_subgrad(self.weights_hidden_output)\n",
    "\n",
    "        return dW2, db2\n",
    "\n",
    "    # ------------------------\n",
    "    # Update weights\n",
    "    # ------------------------\n",
    "    def update(self, dW2, db2, lr=1e-3):\n",
    "        \"\"\"\n",
    "        Gradient descent step on hidden->output layer\n",
    "        \"\"\"\n",
    "        self.weights_hidden_output -= lr * dW2\n",
    "        self.bias_output -= lr * db2\n",
    "\n",
    "    # ------------------------\n",
    "    # Evaluate\n",
    "    # ------------------------\n",
    "    def evaluate_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Return MSE + L1 penalty for the forward pass.\n",
    "        MSE = 0.5 * mean( (pred - y)^2 )\n",
    "        plus L1 = lambda * sum(|W2|)\n",
    "        ignoring W1 since not trained.\n",
    "        \"\"\"\n",
    "        pred = self.forward(X)\n",
    "        mse = 0.5 * np.mean((pred - y)**2)\n",
    "        l1_term = self.l1_lambda * np.sum(np.abs(self.weights_hidden_output))\n",
    "        return mse + l1_term\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Just forward pass, ignoring state variables\n",
    "        \"\"\"\n",
    "        hidden = self.activation(X.dot(self.weights_input_hidden) + self.bias_hidden)\n",
    "        # linear output\n",
    "        output = hidden.dot(self.weights_hidden_output) + self.bias_output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d06f225c-3a21-44a3-88d5-50e49dd20375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(d=100, N=1000, hidden_dim=50, sigma=0.1, seed=None):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset following the described process.\n",
    "\n",
    "    Parameters:\n",
    "    d (int): Number of input features.\n",
    "    N (int): Number of samples.\n",
    "    hidden_dim (int): Number of hidden neurons.\n",
    "    sigma (float): Standard deviation of noise.\n",
    "    seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    X (ndarray): Input feature matrix of shape (d, N).\n",
    "    y (ndarray): Target values of shape (N,).\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Generate input features X, half from normal and half from uniform distribution\n",
    "    X = np.zeros((N, d))\n",
    "    half_d = d // 2\n",
    "    X[:, :half_d] = np.random.normal(0, 1, (N, half_d))  # Normal distribution\n",
    "    X[:, half_d:] = np.random.uniform(-1, 1, (N, d - half_d))  # Uniform distribution\n",
    "\n",
    "\n",
    "    # Generate random weight matrices and bias vector\n",
    "    W1 = np.random.randn(hidden_dim, d)\n",
    "    b1 = np.random.randn(1, hidden_dim) \n",
    "    W2_star = np.random.randn(hidden_dim, 1) \n",
    "\n",
    "\n",
    "    # Compute hidden layer activation\n",
    "    hidden_activation = np.tanh(X @ W1.T + b1)  # Now correct: (N, hidden_dim)\n",
    "\n",
    "    # Compute target values with noise\n",
    "    noise = np.random.normal(0, sigma, (N,1))\n",
    "    y = hidden_activation @ W2_star + noise  # Now correctly (N, 1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8313e7b7-1cf1-4b2e-a803-9974fd05e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_smoothed_gradient(\n",
    "    elm,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs,\n",
    "    base_lr,\n",
    "    momentum_coef, #Ok\n",
    "    lambda_reg=0.01, # We can test more values\n",
    "    mu_init=1e-2, # it should be epsilon/mu but it is the rule of thumbs for defining\\mu\n",
    "    mu_decay=0.5,\n",
    "    L_mse_estimate=1.0,\n",
    "    early_stop_patience=20,\n",
    "    early_stop_tol=1e-4,\n",
    "    gradient_norm_threshold=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the ELM model using a smoothed L1 gradient approach with dynamic mu and LR.\n",
    "\n",
    "    Instead of the non-differentiable L1 norm,\n",
    "    we replace |W2| by sqrt(W2^2 + mu^2),\n",
    "    whose subgradient wrt W2 is:\n",
    "         W2 / sqrt(W2^2 + mu^2).\n",
    "\n",
    "    We also adjust the smoothing parameter mu and the learning rate lr\n",
    "    each epoch to stay aligned with typical smoothing and momentum guidelines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    elm : ELM\n",
    "        Must provide:\n",
    "            - weights_hidden_output (numpy array)\n",
    "            - bias_output (numpy array)\n",
    "            - forward(X)\n",
    "            - backward(X, y) -> (dW2, dB2)\n",
    "            - evaluate_loss(X, y)\n",
    "    X_train, y_train : np.ndarray\n",
    "        Training data, shape is consistent with the ELM design.\n",
    "    epochs : int\n",
    "        Number of epochs to train.\n",
    "    base_lr : float\n",
    "        Base (initial) learning rate factor.\n",
    "    momentum_coef : float\n",
    "        Momentum coefficient in [0, 1).\n",
    "    lambda_reg : float\n",
    "        L1 regularization strength for W2.\n",
    "    mu_init : float\n",
    "        Initial smoothing parameter, typically ~1e-2 or 1e-3.\n",
    "    mu_decay_gamma : float\n",
    "        Exponent for the decay schedule, e.g. 0.5 for inverse-sqrt, 1.0 for inverse-linear.\n",
    "    L_mse_estimate : float\n",
    "        Estimated Lipschitz constant for the MSE gradient.\n",
    "    early_stop_patience : int\n",
    "        Number of epochs with no significant improvement before early stop.\n",
    "    early_stop_tol : float\n",
    "        Relative improvement threshold for early stop check.\n",
    "    gradient_norm_threshold : float or None\n",
    "        If not None, stop when the total gradient norm < threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loss_history : list of floats\n",
    "        Training loss per epoch.\n",
    "    \"\"\"\n",
    "    # Momentum buffers\n",
    "    v_weights = np.zeros_like(elm.weights_hidden_output)\n",
    "    v_bias = np.zeros_like(elm.bias_output)\n",
    "\n",
    "    train_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Update smoothing parameter mu (e.g. inverse-sqrt decay)\n",
    "        mu_k = mu_init / ((epoch + 1) ** mu_decay)\n",
    "\n",
    "        # The effective Lipschitz for the smoothed L1 portion ~ lambda_reg / mu_k\n",
    "        L_smooth_l1 = lambda_reg / max(mu_k, 1e-12)\n",
    "\n",
    "        # Combine with the MSE Lipschitz estimate\n",
    "        L_eff = L_mse_estimate + L_smooth_l1\n",
    "\n",
    "        # Here, we use a simple fixed base_lr to keep code concise.\n",
    "        lr_k = base_lr\n",
    "\n",
    "        # Forward pass\n",
    "        elm.forward(X_train)\n",
    "        train_loss = elm.evaluate_loss(X_train, y_train)\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        # Compute MSE gradient\n",
    "        dW2_mse, db2_mse = elm.backward(X_train, y_train)\n",
    "\n",
    "        # Smoothed L1 gradient for W2\n",
    "        W2_smooth = elm.weights_hidden_output\n",
    "        denom = np.sqrt(W2_smooth * W2_smooth + mu_k * mu_k)\n",
    "        denom = np.where(denom < 1e-12, 1e-12, denom)  # avoid dividing by zero\n",
    "        dW2_smooth_l1 = lambda_reg * (W2_smooth / denom)\n",
    "\n",
    "        # Combine gradients\n",
    "        dW2_total = dW2_mse + dW2_smooth_l1\n",
    "        db2_total = db2_mse  # no L1 penalty on bias\n",
    "\n",
    "        # Optional gradient norm stopping criterion\n",
    "        if gradient_norm_threshold is not None:\n",
    "            grad_norm = np.linalg.norm(dW2_total) + np.linalg.norm(db2_total)\n",
    "            if grad_norm < gradient_norm_threshold:\n",
    "                print(f\"[Early Stop] Gradient norm ({grad_norm:.2e}) below threshold.\")\n",
    "                break\n",
    "\n",
    "        # Heavy-ball momentum update\n",
    "        v_weights = momentum_coef * v_weights - lr_k * dW2_total\n",
    "        v_bias    = momentum_coef * v_bias    - lr_k * db2_total\n",
    "\n",
    "        # Apply updates\n",
    "        elm.weights_hidden_output += v_weights\n",
    "        elm.bias_output           += v_bias\n",
    "\n",
    "    return train_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "29f0b992-2cb9-4c75-9ee0-d08f7bbba7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestOptimizerELM:\n",
    "    def __init__(self, elm_model, optimizer, loss_function=None, grad_function=None):\n",
    "      \"\"\"\n",
    "      Generalized optimizer testing framework.\n",
    "\n",
    "      Parameters:\n",
    "      - elm_model: An instance of an ELM model.\n",
    "      - optimizer: A function that performs optimization (e.g., Momentum, Adam).\n",
    "      - loss_function: The loss function to optimize (default: Ackley function).\n",
    "      - grad_function: The gradient of the loss function (default: Ackley gradient).\n",
    "      \"\"\"\n",
    "      self.elm = elm_model\n",
    "      self.optimizer = optimizer\n",
    "      self.loss_function = loss_function or self.ackley\n",
    "      self.grad_function = grad_function or self.ackley_grad\n",
    "\n",
    "    @staticmethod\n",
    "    def ackley(x):\n",
    "      \"\"\"Computes the Ackley function.\"\"\"\n",
    "      a, b, c = 20, 0.2, 2 * np.pi\n",
    "      n = len(x)\n",
    "      sum1 = np.sum(x**2)\n",
    "      sum2 = np.sum(np.cos(c * x))\n",
    "      term1 = -a * np.exp(-b * np.sqrt(sum1 / n))\n",
    "      term2 = -np.exp(sum2 / n)\n",
    "      return term1 + term2 + a + np.e\n",
    "\n",
    "    @staticmethod\n",
    "    def ackley_grad(x):\n",
    "      \"\"\"Computes the gradient of the Ackley function.\"\"\"\n",
    "      a, b, c = 20, 0.2, 2 * np.pi\n",
    "      n = len(x)\n",
    "      term1 = (b / np.sqrt(n * np.sum(x**2))) * x * a * np.exp(-b * np.sqrt(np.sum(x**2) / n))\n",
    "      term2 = (c / n) * np.sin(c * x) * np.exp(np.sum(np.cos(c * x)) / n)\n",
    "      return term1 + term2\n",
    "\n",
    "    def evaluate_loss(self):\n",
    "      \"\"\"Evaluates the loss function on the model's weights.\"\"\"\n",
    "      return self.loss_function(self.elm.weights_hidden_output.flatten())\n",
    "\n",
    "    def compute_gradient(self):\n",
    "      \"\"\"Computes gradients for optimization.\"\"\"\n",
    "      return self.grad_function(self.elm.weights_hidden_output.flatten()).reshape(self.elm.weights_hidden_output.shape)\n",
    "\n",
    "    def test_optimizer(self, X_train, y_train, epochs=500, lr=0.01, momentum_coef=0.9):\n",
    "      \"\"\"\n",
    "      Tests the optimizer by minimizing the loss function.\n",
    "\n",
    "      Parameters:\n",
    "      - X_train, y_train: Training dataset (can be dummy).\n",
    "      - epochs: Number of training iterations.\n",
    "      - lr: Learning rate.\n",
    "      - momentum_coef: Momentum coefficient (if applicable).\n",
    "      \"\"\"\n",
    "      train_loss_history = self.optimizer(self.elm, X_train, y_train, epochs, lr, momentum_coef)\n",
    "\n",
    "      # Visualization\n",
    "      import matplotlib.pyplot as plt\n",
    "      plt.figure(figsize=(10, 6))\n",
    "      plt.plot(train_loss_history, label=\"Optimizer Performance on Ackley\")\n",
    "      plt.xlabel(\"Epoch\")\n",
    "      plt.ylabel(\"Loss\")\n",
    "      plt.title(\"Convergence of Optimizer on Ackley Function\")\n",
    "      plt.legend()\n",
    "      plt.grid()\n",
    "      plt.show()\n",
    "\n",
    "      final_loss = train_loss_history[-1]\n",
    "      final_weights = self.elm.weights_hidden_output.flatten()\n",
    "\n",
    "      print(f\"\\nFinal loss: {final_loss:.6f} (Expected: ~0)\")\n",
    "      print(f\"Final weight values: {final_weights} (Expected: Close to [0,0])\")\n",
    "\n",
    "      # Final correctness check\n",
    "      assert final_loss < 1e-2, \"Test failed: Final loss is too high!\"\n",
    "      assert np.linalg.norm(final_weights) < 1e-2, \"Test failed: Final weights are not near zero!\"\n",
    "\n",
    "      print(\"✅ Test Passed: Optimizer correctly minimizes the given function!\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "class ELM_Ackley(ELM):\n",
    "    def __init__(self, input_size, hidden_size, output_size, seed=None):\n",
    "      super().__init__(input_size, hidden_size, output_size, activation='relu', seed=seed)\n",
    "\n",
    "    def evaluate_loss(self, X, y=None):\n",
    "      return TestOptimizerELM.ackley(self.weights_hidden_output.flatten())\n",
    "\n",
    "    def backward(self, X, y=None):\n",
    "      grad = TestOptimizerELM.ackley_grad(self.weights_hidden_output.flatten()).reshape(self.weights_hidden_output.shape)\n",
    "      return grad, np.zeros_like(self.bias_output)\n",
    "\n",
    "class ELM_Quadratic(ELM):\n",
    "\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, seed=None):\n",
    "        # We reuse the base ELM's constructor for random initialization,\n",
    "        # but we won't actually use the forward pass for anything. \n",
    "        super().__init__(input_size, hidden_size, output_size, activation='relu', seed=seed)\n",
    "\n",
    "    def evaluate_loss(self, X, y=None):\n",
    "\n",
    "        # shape of weights_hidden_output = (hidden_size, output_size)\n",
    "        W2 = self.weights_hidden_output\n",
    "        return 0.5 * np.sum(W2 * W2)\n",
    "\n",
    "    def backward(self, X, y=None):\n",
    "\n",
    "        grad_w2 = self.weights_hidden_output  # shape = (hidden_size, output_size)\n",
    "        grad_b2 = np.zeros_like(self.bias_output)  # no contribution in the loss\n",
    "        return grad_w2, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "75e6e08e-bb0b-4ec7-a05e-70754d4d9bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running update equation test on quadratic function:\n",
      "Expected weights:\n",
      "[[0.99]\n",
      " [0.99]\n",
      " [0.99]\n",
      " [0.99]\n",
      " [0.99]]\n",
      "Actual weights:\n",
      "[[0.9899]\n",
      " [0.9899]\n",
      " [0.9899]\n",
      " [0.9899]\n",
      " [0.9899]]\n",
      "Update error: 0.000224\n",
      "\n",
      "Running convergence test on quadratic function:\n",
      "Initial loss: 2.115035\n",
      "Final loss: 0.000000\n",
      "\n",
      "Running convergence test on Ackley function:\n",
      "Initial Ackley loss: 3.048249\n",
      "Final Ackley loss: 0.004254\n",
      "\n",
      "Running gradient norm decrease test on quadratic function:\n",
      "Initial gradient norm: 2.056713\n",
      "Final gradient norm: 0.000024\n",
      "\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Generic Optimizer Tester\n",
    "# ------------------------\n",
    "class OptimizerTester:\n",
    "    def __init__(self, optimizer_func):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          optimizer_func : A function that trains an ELM instance and returns\n",
    "                           the training loss history. It must have the same\n",
    "                           interface as the current train_momentum function.\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer_func\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Unit test: Update Equations\n",
    "    # -------------------------------\n",
    "    def test_update_equations_on_quadratic(self):\n",
    "        print(\"Running update equation test on quadratic function:\")\n",
    "        input_size, hidden_size, output_size = 10, 5, 1\n",
    "        elm = ELM_Quadratic(input_size, hidden_size, output_size, seed=123)\n",
    "        # For a deterministic check, set weights to ones.\n",
    "        initial_weights = np.ones((hidden_size, output_size))\n",
    "        elm.weights_hidden_output = initial_weights.copy()\n",
    "\n",
    "        # Use one update step with lr=0.01 and momentum_coef=0.9.\n",
    "        lr = 0.01\n",
    "        momentum_coef = 0.9\n",
    "        epochs = 1\n",
    "        X_dummy = np.zeros((10, input_size))\n",
    "        y_dummy = np.zeros((10, output_size))\n",
    "\n",
    "        _ = self.optimizer(elm, X_dummy, y_dummy, epochs, lr, momentum_coef)\n",
    "\n",
    "        # Expected update:\n",
    "        # For the quadratic loss f(W)=0.5*||W||^2, the gradient is W.\n",
    "        # With one iteration: v = -lr * grad = -0.01 * 1 = -0.01,\n",
    "        # so the new weight should be 1 - 0.01 = 0.99.\n",
    "        expected_weights = 0.99 * initial_weights\n",
    "        actual_weights = elm.weights_hidden_output\n",
    "        error = np.linalg.norm(actual_weights - expected_weights)\n",
    "\n",
    "        print(f\"Expected weights:\\n{expected_weights}\")\n",
    "        print(f\"Actual weights:\\n{actual_weights}\")\n",
    "        print(f\"Update error: {error:.6f}\\n\")\n",
    "        assert error < 1e-1, \"Update equations test failed!\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Convergence Test on a Quadratic Function\n",
    "    # --------------------------------------------------\n",
    "    def test_convergence_quadratic(self):\n",
    "        print(\"Running convergence test on quadratic function:\")\n",
    "        input_size, hidden_size, output_size = 10, 5, 1\n",
    "        elm = ELM_Quadratic(input_size, hidden_size, output_size, seed=123)\n",
    "        # Start with random weights.\n",
    "        elm.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "        X_dummy = np.zeros((10, input_size))\n",
    "        y_dummy = np.zeros((10, output_size))\n",
    "\n",
    "        epochs = 200\n",
    "        lr = 0.01\n",
    "        momentum_coef = 0.9\n",
    "        losses = self.optimizer(elm, X_dummy, y_dummy, epochs, lr, momentum_coef)\n",
    "\n",
    "        print(f\"Initial loss: {losses[0]:.6f}\")\n",
    "        print(f\"Final loss: {losses[-1]:.6f}\\n\")\n",
    "        # Since the minimum is zero, the final loss should be very small.\n",
    "        assert losses[-1] < 1e-6, \"Quadratic convergence test failed!\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Convergence Test on the Ackley Function\n",
    "    # --------------------------------------------------\n",
    "    def test_convergence_ackley(self):\n",
    "        print(\"Running convergence test on Ackley function:\")\n",
    "        input_size, hidden_size, output_size = 10, 20, 1\n",
    "        elm = ELM_Ackley(input_size, hidden_size, output_size, seed=123)\n",
    "\n",
    "        # For the Ackley function, the test is that the loss decreases relative to the start.\n",
    "        X_dummy = np.zeros((10, input_size))\n",
    "        y_dummy = np.zeros((10, output_size))\n",
    "\n",
    "        epochs = 200\n",
    "        lr = 0.01\n",
    "        momentum_coef = 0.9\n",
    "        losses = self.optimizer(elm, X_dummy, y_dummy, epochs, lr, momentum_coef)\n",
    "\n",
    "        print(f\"Initial Ackley loss: {losses[0]:.6f}\")\n",
    "        print(f\"Final Ackley loss: {losses[-1]:.6f}\\n\")\n",
    "        assert losses[-1] < losses[0], \"Ackley convergence test failed: Loss did not decrease!\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Test: Gradient Norm Decrease on Quadratic Function\n",
    "    # --------------------------------------------------\n",
    "    def test_gradient_norm_decrease_quadratic(self):\n",
    "        print(\"Running gradient norm decrease test on quadratic function:\")\n",
    "        input_size, hidden_size, output_size = 10, 5, 1\n",
    "        elm = ELM_Quadratic(input_size, hidden_size, output_size, seed=123)\n",
    "        elm.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "        X_dummy = np.zeros((10, input_size))\n",
    "        y_dummy = np.zeros((10, output_size))\n",
    "\n",
    "        # Compute the initial gradient norm.\n",
    "        initial_grad, _ = elm.backward(X_dummy, y_dummy)\n",
    "        initial_grad_norm = np.linalg.norm(initial_grad)\n",
    "\n",
    "        # Run the optimizer for a number of epochs.\n",
    "        epochs = 200\n",
    "        lr = 0.01\n",
    "        momentum_coef = 0.9\n",
    "        _ = self.optimizer(elm, X_dummy, y_dummy, epochs, lr, momentum_coef)\n",
    "\n",
    "        # Compute the final gradient norm.\n",
    "        final_grad, _ = elm.backward(X_dummy, y_dummy)\n",
    "        final_grad_norm = np.linalg.norm(final_grad)\n",
    "\n",
    "        print(f\"Initial gradient norm: {initial_grad_norm:.6f}\")\n",
    "        print(f\"Final gradient norm: {final_grad_norm:.6f}\\n\")\n",
    "        assert final_grad_norm < initial_grad_norm, \"Gradient norm did not decrease!\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # Run all tests\n",
    "    # -------------------------------\n",
    "    def run_all_tests(self):\n",
    "        self.test_update_equations_on_quadratic()\n",
    "        self.test_convergence_quadratic()\n",
    "        self.test_convergence_ackley()\n",
    "        self.test_gradient_norm_decrease_quadratic()\n",
    "        print(\"All tests passed successfully!\")\n",
    "\n",
    "# Example usage:\n",
    "# Assume that train_momentum is defined as in your current implementation.\n",
    "# To test the momentum-based optimizer:\n",
    "tester = OptimizerTester(train_smoothed_gradient)\n",
    "tester.run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76678261-62f5-41d9-8f17-2a6436c46c49",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4eddc-d16d-44a8-b461-80ff28956100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Momentum] λ=0.0001, lr=1e-05, momentum=0.8 => Train Loss: 18.8217, Val Loss: 18.7072, Sparsity: 0.00%, Total Time: 4.74s\n",
      "[Momentum] λ=0.0001, lr=1e-05, momentum=0.9 => Train Loss: 18.7096, Val Loss: 18.4916, Sparsity: 0.00%, Total Time: 4.69s\n",
      "[Momentum] λ=0.0001, lr=1e-05, momentum=0.99 => Train Loss: 17.3265, Val Loss: 16.2613, Sparsity: 0.00%, Total Time: 4.80s\n",
      "[Momentum] λ=0.0001, lr=0.001, momentum=0.8 => Train Loss: 14.3655, Val Loss: 13.6783, Sparsity: 0.00%, Total Time: 4.71s\n",
      "[Momentum] λ=0.0001, lr=0.001, momentum=0.9 => Train Loss: 13.6726, Val Loss: 13.4740, Sparsity: 0.00%, Total Time: 4.67s\n",
      "[Momentum] λ=0.0001, lr=0.001, momentum=0.99 => Train Loss: 13.4701, Val Loss: 13.4479, Sparsity: 0.00%, Total Time: 4.81s\n",
      "[Momentum] λ=0.0001, lr=0.01, momentum=0.8 => Train Loss: 13.4505, Val Loss: 13.4482, Sparsity: 0.00%, Total Time: 4.79s\n",
      "[Momentum] λ=0.0001, lr=0.01, momentum=0.9 => Train Loss: 13.4482, Val Loss: 13.4477, Sparsity: 0.00%, Total Time: 4.67s\n",
      "[Momentum] λ=0.0001, lr=0.01, momentum=0.99 => Train Loss: 13.4669, Val Loss: 13.4478, Sparsity: 0.00%, Total Time: 4.67s\n",
      "[Momentum] λ=0.0001, lr=0.05, momentum=0.8 => Train Loss: 13.4477, Val Loss: 13.4477, Sparsity: 0.00%, Total Time: 4.80s\n",
      "[Momentum] λ=0.0001, lr=0.05, momentum=0.9 => Train Loss: 13.4477, Val Loss: 13.4477, Sparsity: 0.00%, Total Time: 4.67s\n",
      "[Momentum] λ=0.0001, lr=0.05, momentum=0.99 => Train Loss: 13.4627, Val Loss: 13.4478, Sparsity: 0.00%, Total Time: 4.68s\n",
      "[Momentum] λ=0.0001, lr=0.1, momentum=0.8 => Train Loss: 13.4477, Val Loss: 13.4477, Sparsity: 0.00%, Total Time: 4.80s\n",
      "[Momentum] λ=0.0001, lr=0.1, momentum=0.9 => Train Loss: 13.4477, Val Loss: 13.4477, Sparsity: 0.00%, Total Time: 4.63s\n",
      "[Momentum] λ=0.0001, lr=0.1, momentum=0.99 => Train Loss: 13.4677, Val Loss: 13.4478, Sparsity: 0.00%, Total Time: 4.65s\n",
      "[Momentum] λ=0.001, lr=1e-05, momentum=0.8 => Train Loss: 18.8294, Val Loss: 18.7148, Sparsity: 0.00%, Total Time: 4.69s\n",
      "[Momentum] λ=0.001, lr=1e-05, momentum=0.9 => Train Loss: 18.7172, Val Loss: 18.4991, Sparsity: 0.00%, Total Time: 4.82s\n",
      "[Momentum] λ=0.001, lr=1e-05, momentum=0.99 => Train Loss: 17.3341, Val Loss: 16.2724, Sparsity: 0.00%, Total Time: 4.63s\n",
      "[Momentum] λ=0.001, lr=0.001, momentum=0.8 => Train Loss: 14.3886, Val Loss: 13.7060, Sparsity: 0.00%, Total Time: 4.86s\n",
      "[Momentum] λ=0.001, lr=0.001, momentum=0.9 => Train Loss: 13.7002, Val Loss: 13.5028, Sparsity: 0.00%, Total Time: 5.23s\n",
      "[Momentum] λ=0.001, lr=0.001, momentum=0.99 => Train Loss: 13.4980, Val Loss: 13.4759, Sparsity: 0.00%, Total Time: 5.02s\n",
      "[Momentum] λ=0.001, lr=0.01, momentum=0.8 => Train Loss: 13.4785, Val Loss: 13.4762, Sparsity: 0.00%, Total Time: 4.72s\n",
      "[Momentum] λ=0.001, lr=0.01, momentum=0.9 => Train Loss: 13.4761, Val Loss: 13.4757, Sparsity: 0.00%, Total Time: 4.91s\n",
      "[Momentum] λ=0.001, lr=0.01, momentum=0.99 => Train Loss: 13.4948, Val Loss: 13.4759, Sparsity: 0.00%, Total Time: 4.66s\n",
      "[Momentum] λ=0.001, lr=0.05, momentum=0.8 => Train Loss: 13.4757, Val Loss: 13.4757, Sparsity: 0.00%, Total Time: 4.85s\n",
      "[Momentum] λ=0.001, lr=0.05, momentum=0.9 => Train Loss: 13.4757, Val Loss: 13.4757, Sparsity: 0.00%, Total Time: 4.87s\n",
      "[Momentum] λ=0.001, lr=0.05, momentum=0.99 => Train Loss: 13.4912, Val Loss: 13.4758, Sparsity: 0.00%, Total Time: 4.87s\n",
      "[Momentum] λ=0.001, lr=0.1, momentum=0.8 => Train Loss: 13.4757, Val Loss: 13.4757, Sparsity: 0.00%, Total Time: 4.71s\n",
      "[Momentum] λ=0.001, lr=0.1, momentum=0.9 => Train Loss: 13.4757, Val Loss: 13.4757, Sparsity: 0.00%, Total Time: 4.68s\n",
      "[Momentum] λ=0.001, lr=0.1, momentum=0.99 => Train Loss: 13.4957, Val Loss: 13.4759, Sparsity: 0.00%, Total Time: 4.83s\n",
      "[Momentum] λ=0.01, lr=1e-05, momentum=0.8 => Train Loss: 18.9059, Val Loss: 18.7906, Sparsity: 0.00%, Total Time: 4.74s\n",
      "[Momentum] λ=0.01, lr=1e-05, momentum=0.9 => Train Loss: 18.7930, Val Loss: 18.5738, Sparsity: 0.00%, Total Time: 4.71s\n",
      "[Momentum] λ=0.01, lr=1e-05, momentum=0.99 => Train Loss: 17.4087, Val Loss: 16.3823, Sparsity: 2.00%, Total Time: 4.99s\n",
      "[Momentum] λ=0.01, lr=0.001, momentum=0.8 => Train Loss: 14.6183, Val Loss: 13.9815, Sparsity: 4.00%, Total Time: 4.66s\n",
      "[Momentum] λ=0.01, lr=0.001, momentum=0.9 => Train Loss: 13.9744, Val Loss: 13.7886, Sparsity: 2.00%, Total Time: 4.63s\n",
      "[Momentum] λ=0.01, lr=0.001, momentum=0.99 => Train Loss: 13.7730, Val Loss: 13.7562, Sparsity: 0.00%, Total Time: 4.80s\n",
      "[Momentum] λ=0.01, lr=0.01, momentum=0.8 => Train Loss: 13.7579, Val Loss: 13.7554, Sparsity: 2.00%, Total Time: 4.73s\n",
      "[Momentum] λ=0.01, lr=0.01, momentum=0.9 => Train Loss: 13.7554, Val Loss: 13.7551, Sparsity: 2.00%, Total Time: 4.59s\n",
      "[Momentum] λ=0.01, lr=0.01, momentum=0.99 => Train Loss: 13.7688, Val Loss: 13.7558, Sparsity: 2.00%, Total Time: 4.66s\n",
      "[Momentum] λ=0.01, lr=0.05, momentum=0.8 => Train Loss: 13.7551, Val Loss: 13.7552, Sparsity: 2.00%, Total Time: 4.92s\n",
      "[Momentum] λ=0.01, lr=0.05, momentum=0.9 => Train Loss: 13.7552, Val Loss: 13.7552, Sparsity: 2.00%, Total Time: 4.63s\n",
      "[Momentum] λ=0.01, lr=0.05, momentum=0.99 => Train Loss: 13.7711, Val Loss: 13.7550, Sparsity: 0.00%, Total Time: 4.72s\n",
      "[Momentum] λ=0.01, lr=0.1, momentum=0.8 => Train Loss: 13.7551, Val Loss: 13.7552, Sparsity: 2.00%, Total Time: 4.80s\n",
      "[Momentum] λ=0.01, lr=0.1, momentum=0.9 => Train Loss: 13.7552, Val Loss: 13.7552, Sparsity: 2.00%, Total Time: 4.79s\n",
      "[Momentum] λ=0.01, lr=0.1, momentum=0.99 => Train Loss: 13.7689, Val Loss: 13.7548, Sparsity: 0.00%, Total Time: 4.70s\n",
      "[Momentum] λ=0.1, lr=1e-05, momentum=0.8 => Train Loss: 19.6492, Val Loss: 19.5062, Sparsity: 0.00%, Total Time: 4.71s\n",
      "[Momentum] λ=0.1, lr=1e-05, momentum=0.9 => Train Loss: 19.5092, Val Loss: 19.2459, Sparsity: 4.00%, Total Time: 4.80s\n",
      "[Momentum] λ=0.1, lr=1e-05, momentum=0.99 => Train Loss: 17.9793, Val Loss: 17.3550, Sparsity: 18.00%, Total Time: 4.80s\n",
      "[Momentum] λ=0.1, lr=0.001, momentum=0.8 => Train Loss: 16.5768, Val Loss: 16.2534, Sparsity: 50.00%, Total Time: 4.70s\n",
      "[Momentum] λ=0.1, lr=0.001, momentum=0.9 => Train Loss: 16.2443, Val Loss: 16.1259, Sparsity: 50.00%, Total Time: 4.90s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Experiment Evaluation Function for Momentum Approach\n",
    "# =============================================================================\n",
    "def evaluate_experiments_momentum(X_train, y_train, X_val, y_val,\n",
    "                                  input_size, hidden_size, output_size,\n",
    "                                  epochs, seed):\n",
    "    \"\"\"\n",
    "    Run experiments over a grid of hyperparameters for the momentum approach.\n",
    "\n",
    "    We vary:\n",
    "      - Learning rate (lr)\n",
    "      - Momentum coefficient (eta)\n",
    "      - Regularization parameter lambda (l1_lambda)\n",
    "      - Gradient noise levels\n",
    "\n",
    "    Metrics collected include:\n",
    "      - Convergence speed (loss curves)\n",
    "      - Final loss values (train and validation)\n",
    "      - Sparsity of W2 (fraction of weights with abs(value) < threshold)\n",
    "      - Computational efficiency (total training time and avg epoch time)\n",
    "      - Robustness (performance variation under different noise levels)\n",
    "    \"\"\"\n",
    "    learning_rates = [1e-5, 0.001, 0.01, 0.05, 0.1]\n",
    "    momentum_coefs = [0.8, 0.9, 0.99]\n",
    "    lambda_values = [1e-4, 1e-3, 1e-2, 1e-1]  # test different L1 regularization strengths\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for lam, lr, momentum in itertools.product(lambda_values, learning_rates, momentum_coefs):\n",
    "        # Initialize the model (same architecture, fixed seed for reproducibility)\n",
    "        elm_model = ELM(input_size, hidden_size, output_size, activation='relu', seed=seed)\n",
    "        elm_model.l1_lambda = lam\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_losses = train_smoothed_gradient(elm_model, X_train, y_train, epochs, lr, momentum, lambda_reg=lam)\n",
    "            \n",
    "        val_losses =  train_smoothed_gradient(elm_model, X_train, y_train, epochs, lr, momentum, lambda_reg=lam)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        avg_epoch_time = total_time / epochs\n",
    "\n",
    "        # Compute sparsity of W2: fraction of weights with abs(value) below a small threshold.\n",
    "        threshold = 1e-3\n",
    "        sparsity = np.mean(np.abs(elm_model.weights_hidden_output) < threshold)\n",
    "\n",
    "        config = ('momentum', lam, lr, momentum)\n",
    "        results[config] = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"total_time\": total_time,\n",
    "            \"avg_epoch_time\": avg_epoch_time,\n",
    "            \"final_sparsity\": sparsity,\n",
    "            \"final_train_loss\": train_losses[-1],\n",
    "            \"final_val_loss\": val_losses[-1]\n",
    "        }\n",
    "        print(f\"[Momentum] λ={lam}, lr={lr}, momentum={momentum} => \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, \"\n",
    "              f\"Sparsity: {sparsity*100:.2f}%, Total Time: {total_time:.2f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Synthetic Data Experiment ---\n",
    "input_size = 100\n",
    "n_samples = 5000\n",
    "hidden_size = 50\n",
    "output_size = 1\n",
    "epochs = 500\n",
    "sigma = 0.1\n",
    "seed = 42\n",
    "X_syn, y_syn = generate_synthetic_data(\n",
    "    d=input_size, # n input features\n",
    "    N=n_samples, # number of samples\n",
    "    hidden_dim=hidden_size, # n hidden layers\n",
    "    sigma=sigma, # std dev of noise\n",
    "    seed=seed)\n",
    "\n",
    "split_index = int(0.8 * n_samples)\n",
    "X_train_syn, y_train_syn = X_syn[:split_index], y_syn[:split_index]\n",
    "X_val_syn, y_val_syn = X_syn[split_index:], y_syn[split_index:]\n",
    "\n",
    "results_synthetic = evaluate_experiments_momentum(\n",
    "    X_train_syn,\n",
    "    y_train_syn,\n",
    "    X_val_syn,\n",
    "    y_val_syn,\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    epochs,\n",
    "    seed)\n",
    "\n",
    "# Plot convergence curves for a subset of experiments (e.g. no gradient noise) for clarity.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for config, metrics in results_synthetic.items():\n",
    "    algo, lam, lr_val, momentum = config\n",
    "    plt.plot(metrics[\"train_losses\"], label=f\"λ={lam}, lr={lr_val}, momentum={momentum}\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Momentum Convergence Curves on Synthetic Data\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Summary table of key metrics.\n",
    "print(\"\\nSummary of Experimental Results (Synthetic Data):\")\n",
    "header = (\"Algorithm\", \"λ\", \"lr\", \"Momentum\", \"Final Train Loss\",\n",
    "          \"Final Val Loss\", \"Sparsity (%)\", \"Total Time (s)\", \"Avg Epoch Time (s)\")\n",
    "print(\"\\t\".join(str(h) for h in header))\n",
    "for config, metrics in results_synthetic.items():\n",
    "    algo, lam, lr_val, momentum = config\n",
    "    print(f\"{algo}\\t{lam:.1e}\\t{lr_val:.3f}\\t{momentum:.2f}\\t\\t\"\n",
    "          f\"{metrics['final_train_loss']:.4f}\\t\\t{metrics['final_val_loss']:.4f}\\t\\t\"\n",
    "          f\"{metrics['final_sparsity']*100:.2f}\\t\\t{metrics['total_time']:.2f}\\t\\t\"\n",
    "          f\"{metrics['avg_epoch_time']:.4f}\")\n",
    "\n",
    "# Print information at the end of the descent\n",
    "# - final value\n",
    "# - difference\n",
    "# - convergence speed\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4be7ed-88b8-4d01-bf41-2e3939bffed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5ab5e-a751-45d8-97aa-4f1a67a4beae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a202a-141f-4ced-a980-bd86d2b9e4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21298e47-e196-47b5-b8a3-cf9953b7c3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
